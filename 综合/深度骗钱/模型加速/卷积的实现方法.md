# 一.引子 #
## 引1:计算方法 ##  
**FLOP/s**:每秒浮点运算数（FLoating Point Operations per Second，FLOP/s）  
**例子**:
    
CPU的配置:
  物理内核\*2  
  内核频率:2.5GHz,(每秒运行2.5×10^9个CPU循环)  
  每个循环可处理32 FLOPs（使用AVX & FMA）  
  CPU理论峰值性能为  
  ![](http://latex.codecogs.com/gif.latex?\\2*2.5*10^9\frac{cycles}{second}*32\frac{FLOP}{cycle}=160GFLOP/s)  
  理论峰值无法实现的原因在于，内存访问同样需要时间

## 引2:存储形式 ##  
逻辑上矩阵/图像/张量是多维度的，实际上它们存储在**线性、一维**的计算机内存中。  
大部分现代深度学习库使用**行主序**作为存储顺序,即同一行的连续元素被存储在相邻位置.  
同时也意味着在线性扫描内存时第一个维度的变化速度最慢  
通常4维张量（如CNN中的张量）的存储顺序是NCHW(batch,channel,height,weight)、NHWC等  
[矩阵存储顺序]


# 二.朴素卷积 #
## 1.朴素卷积(Naive Convolution) ##  
  
    for filter in (0,num_filters):
       for channel in (0,input_channels):
          for out_h in (0,output_height):  
              for out_w in (0,output_width):
                  for k_h in (0,kernel_height):
                      for k_w in (0,kernel_width):
                          output[filter, channel, out_h, out_h] +=   
                          kernel[filter, channel, k_h, k_w] *    
                          input[channel, out_h + k_h, out_w + k_w]

# 二.im2col #
## 2.矩阵乘法和卷积:im2col ##  
https://sahnimanas.github.io/post/anatomy-of-a-high-performance-convolution/  
下图展示了一个正常的3x3卷积：  
[3x3卷积]
下图展示的是该卷积运算被实现为矩阵相乘的形式。右侧的矩阵是im2col的结果，它需要从原始图像中复制像素才能得以构建。左侧的矩阵是卷积的权重，它们已经以这种形式储存在内存中。  
[转化为矩阵相乘]  
矩阵乘积直接得出了卷积输出，无需额外「转换」至原始形式  
*出于视觉简洁考虑，此处将每个图像块作为独立的个体进行展示。而在现实中，不同图像块之间通常会有重叠，因而im2col可能导致内存重叠。生成im2col 缓冲（im2col buffer）和过多内存（inflated memory）所花费的时间必须通过GEMM实现的加速来抵消。???*

## 矩阵相乘加速(GEMM) ##  
加速目标:C_{M*N}+=A_{M*K}*B_{K*N}  

**基础矩阵相乘时间**  
    
    for i in 0..M:    
        for j in 0..N:        
            for k in 0..K:            
                C[i, j] += A[i, k] * B[k, j]
    
**内存和cpu计算上的加速**  
内存RAM速度慢于cpu的缓存.  
CPU访问内存时会将临近数据加载(locality of reference)  
[加速前]
对于A:  
一旦我们找到 A[i, k]，则它在该行中的下一个元素A[i, k+1]已经被缓存了.  
对于B:  
列的下一个元素并未出现在缓存中，即出现了缓存缺失（cache miss）。这时尽管获取到了数据，CPU也出现了一次停顿。  
获取数据后，缓存同时也被 B 中同一行的其他元素填满。我们实际上并不会使用到它们，因此它们很快就会被删除。多次迭代后，当我们需要那些元素时，我们将再次获取它们。我们在用实际上不需要的值污染缓存。  
**重新修改loop，以充分利用缓存能力。**  
    
    for i in 0..M:    
        for k in 0..K:        
            for j in 0..N:
[加速后]  

**平铺(Tiling)**  
另一个缓存问题:  
对于A中的每一行，我们针对B中所有列进行循环。而对于 B 中的每一步，我们会在缓存中加载一些新的列，去除一些旧的列。当到达A的下一行时，我们仍旧重新从第一列开始循环。我们不断在缓存中添加和删除同样的数据，即缓存颠簸（cache thrashing）。  
如果所有数据均适应缓存，则颠簸不会出现。如果我们处理的是小型矩阵，则它们会舒适地待在缓存里，不用经历重复的驱逐。庆幸的是，我们可以将矩阵相乘分解为子矩阵。要想计算 C 的r×c平铺，我们仅需要A的r行和B的c列。接下来，我们将 C 分解为6x16的平铺：  
    
    C(x, y) += A(k, y) * B(x, k);
    C.update().tile(x, y, xo, yo, xi, yi, 6, 16)
    /*in pseudocode:for xo in 0..N/16:    
    for yo in 0..M/6:        
        for yi in 6:            
            for xi in 0..16:                
                for k in 0..K:                    
                    C(...) = ...*/  
我们将x,y 维度分解为外侧的xo,yo和内侧的xi,yi。我们将为该6x16 block优化micro-kernel（即xi,yi），然后在所有block上运行micro-kernel（通过xo,yo进行迭代）。  
**单指令流多数据流加速**  
向量化 & FMA大部分现代CPU支持SIMD（Single Instruction Multiple Data，单指令流多数据流）。在同一个CPU循环中，SIMD可在多个值上同时执行相同的运算/指令（如加、乘等）。如果我们在4个数据点上同时运行SIMD指令，就会直接实现4倍的加速。  

[SIMD]  


**FMA（Fused Multiply-Add）**  


**多线程**  
对于非常小的规模而言，性能反而下降了。这是因为工作负载很小，线程花费较少的时间来处理工作负载，而花费较多的时间同步其他线程。  



**展开（Unrolling）**  
循环使我们避免重复写同样代码的痛苦，但同时它也引入了一些额外的工作，如检查循环终止、更新循环计数器、指针运算等。如果手动写出重复的循环语句并展开循环，我们就可以减少这一开销。  
展开是几乎完全被编译器负责的另一种优化方式，除了我们想要更多掌控的micro-kernel  

    C.update().tile(x, y, xo, yo, xi, yi, 6, 16).reorder(xi, yi, k, xo, yo).vectorize(xi, 8).unroll(xi).unroll(yi)
    /*in pseudocode:for xo in 0..N/16:    
    for parallel yo:        
        for k in 0..K:            
            C(xi:xi+8, yi+0)            
            C(xi:xi+8, yi+1)            
            ...            
            C(xi:xi+8, yi+5)            
            C(xi+8:xi+16, yi+0)            
            C(xi+8:xi+16, yi+1)            
            ...            
            C(xi+8:xi+16, yi+5)*/
    
    
    



# 三.Winograd #  
https://arxiv.org/abs/1509.09308  
https://hey-yahei.cn/2019/08/21/winograd_convolution/#%E5%B5%8C%E5%A5%97%E5%AE%9E%E7%8E%B0    
更适合小卷积，如3\*3的卷积
**核心是减少矩阵的乘法运算次数**  
例子：
先以1维卷积为例，输入信号为![](https://latex.codecogs.com/gif.latex?d=\left[&space;\begin{array}{llll}{d_{0}}&space;&&space;{d_{1}}&space;&&space;{d_{2}}&space;&&space;{d_{3}}\end{array}\right]^{T})，卷积核为![](https://latex.codecogs.com/gif.latex?g=\left[&space;\begin{array}{lll}{g_{0}}&space;&&space;{g_{1}}&space;&&space;{g_{2}}\end{array}\right]^{T})，则卷积可写成如下矩阵乘法形式：  
<img src="https://latex.codecogs.com/gif.latex?F(2,3)&space;=&space;\begin{bmatrix}&space;{d_{0}}&space;&&space;{d_{1}}&space;&&space;{d_{2}}\\&space;{d_{1}}&&space;{d_{2}}&space;&&space;{d_{3}}&space;\end{bmatrix}&space;\begin{bmatrix}&space;{g_{0}}&space;&&space;{g_{1}}&space;&&space;{g_{2}}&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;{r_{0}}&space;&{r_{1}}&space;\end{bmatrix}" title="F(2,3) = \begin{bmatrix} {d_{0}} & {d_{1}} & {d_{2}}\\ {d_{1}}& {d_{2}} & {d_{3}} \end{bmatrix} \begin{bmatrix} {g_{0}} & {g_{1}} & {g_{2}} \end{bmatrix} = \begin{bmatrix} {r_{0}} &{r_{1}} \end{bmatrix}" />  
如果是一般的矩阵乘法，则需要6次乘法和4次加法，如下：  
<img src="https://latex.codecogs.com/gif.latex?\begin{array}{l}{r_{0}=\left(d_{0}&space;\cdot&space;g_{0}\right)&plus;\left(d_{1}&space;\cdot&space;g_{1}\right)&plus;\left(d_{2}&space;\cdot&space;g_{2}\right)}&space;\\&space;{r_{1}=\left(d_{1}&space;\cdot&space;g_{0}\right)&plus;\left(d_{2}&space;\cdot&space;g_{1}\right)&plus;\left(d_{3}&space;\cdot&space;g_{2}\right)}\end{array}" title="\begin{array}{l}{r_{0}=\left(d_{0} \cdot g_{0}\right)+\left(d_{1} \cdot g_{1}\right)+\left(d_{2} \cdot g_{2}\right)} \\ {r_{1}=\left(d_{1} \cdot g_{0}\right)+\left(d_{2} \cdot g_{1}\right)+\left(d_{3} \cdot g_{2}\right)}\end{array}" />   
但是，卷积运算中输入信号转换成的矩阵不是任意矩阵，其中**有规律地分布着大量的重复元素**，比如第1行和第2行的d1和d2，卷积转换成的矩阵乘法比一般矩阵乘法的问题域更小，这就让优化存在了可能:    
Winograd:    
<img src="https://latex.codecogs.com/gif.latex?F(2,3)=\left[&space;\begin{array}{lll}{d_{0}}&space;&&space;{d_{1}}&space;&&space;{d_{2}}&space;\\&space;{d_{1}}&space;&&space;{d_{2}}&space;&&space;{d_{3}}\end{array}\right]&space;\left[&space;\begin{array}{l}{g_{0}}&space;\\&space;{g_{1}}&space;\\&space;{g_{2}}\end{array}\right]=\left[&space;\begin{array}{c}{m_{1}&plus;m_{2}&plus;m_{3}}&space;\\&space;{m_{2}-m_{3}-m_{4}}\end{array}\right]" title="F(2,3)=\left[ \begin{array}{lll}{d_{0}} & {d_{1}} & {d_{2}} \\ {d_{1}} & {d_{2}} & {d_{3}}\end{array}\right] \left[ \begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[ \begin{array}{c}{m_{1}+m_{2}+m_{3}} \\ {m_{2}-m_{3}-m_{4}}\end{array}\right]" />  
其中：   
<img src="https://latex.codecogs.com/gif.latex?\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right)&space;g_{0}}&space;&&space;{m_{2}=\left(d_{1}&plus;d_{2}\right)&space;\frac{g_{0}&plus;g_{1}&plus;g_{2}}{2}}&space;\\&space;{m_{4}=\left(d_{1}-d_{3}\right)&space;g_{2}}&space;&&space;{m_{3}=\left(d_{2}-d_{1}\right)&space;\frac{g_{0}-g_{1}&plus;g_{2}}{2}}\end{array}" title="\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right) g_{0}} & {m_{2}=\left(d_{1}+d_{2}\right) \frac{g_{0}+g_{1}+g_{2}}{2}} \\ {m_{4}=\left(d_{1}-d_{3}\right) g_{2}} & {m_{3}=\left(d_{2}-d_{1}\right) \frac{g_{0}-g_{1}+g_{2}}{2}}\end{array}" />  
用矩阵运算可以表示成：  
![](https://latex.codecogs.com/gif.latex?Y=A^{T}\left[(G&space;g)&space;\odot\left(B^{T}&space;d\right)\right])   
其中https://latex.codecogs.com/gif.latex?\odot表示点乘，而
<img src="https://latex.codecogs.com/gif.latex?B^{T}=\left[\begin{array}{rrrr}{1}&space;&&space;{0}&space;&&space;{-1}&space;&&space;{0}&space;\\&space;{0}&space;&&space;{1}&space;&&space;{1}&space;&&space;{0}&space;\\&space;{0}&space;&&space;{-1}&space;&&space;{1}&space;&&space;{0}&space;\\&space;{0}&space;&&space;{1}&space;&&space;{0}&space;&&space;{-1}\end{array}\right],&space;G=\left[\begin{array}{rrr}{1}&space;&&space;{0}&space;&&space;{0}&space;\\&space;{\frac{1}{2}}&space;&&space;{\frac{1}{2}}&space;&&space;{\frac{1}{2}}&space;\\&space;{\frac{1}{2}}&space;&&space;{-\frac{1}{2}}&space;&&space;{\frac{1}{2}}&space;\\&space;{0}&space;&&space;{0}&space;&&space;{1}\end{array}\right],&space;A^{T}=\left[\begin{array}{rrrr}{1}&space;&&space;{1}&space;&&space;{1}&space;&&space;{0}&space;\\&space;{0}&space;&&space;{1}&space;&&space;{-1}&space;&&space;{-1}\end{array}\right]" title="B^{T}=\left[\begin{array}{rrrr}{1} & {0} & {-1} & {0} \\ {0} & {1} & {1} & {0} \\ {0} & {-1} & {1} & {0} \\ {0} & {1} & {0} & {-1}\end{array}\right], G=\left[\begin{array}{rrr}{1} & {0} & {0} \\ {\frac{1}{2}} & {\frac{1}{2}} & {\frac{1}{2}} \\ {\frac{1}{2}} & {-\frac{1}{2}} & {\frac{1}{2}} \\ {0} & {0} & {1}\end{array}\right], A^{T}=\left[\begin{array}{rrrr}{1} & {1} & {1} & {0} \\ {0} & {1} & {-1} & {-1}\end{array}\right]" />   
1.由于g是固定的FIR滤波器参数，那么Gg可以提前计算并得到一个 4×1 的列向量  
2.由于d是变化的输入序列，所以每次计算FIR的时候都需要对输入d做一个变换https://latex.codecogs.com/gif.latex?B^Td，得到一个 4×1 的列向量，这个过程需要4次加法（注意https://latex.codecogs.com/gif.latex?B^Td矩阵的元素值）    
3.然后Gg和https://latex.codecogs.com/gif.latex?B^Td进行点乘，共计4次乘法  
4.最后AT与[(Gg)⊙(https://latex.codecogs.com/gif.latex?B^Td)]做乘法，共计4次加法   
过程1可以提前完成，变换过程2和计算过程3、4共计4次乘法和8次加法，相比于直接FIR的6次乘法、4次加法，乘法次数下降为原来的23（推广到一般情况，直接FIR跟Winograd的乘法次数分别是m×r和m+r−1）。  
但天下没有免费的午餐，既然速度得到提升，那么肯定需要付出代价——算法的加速往往是需要以额外的空间为代价的：原先FIR只需要存储3个参数g，但现在需要存储4个参数Gg（推广到一般情况，分别是r和m+r−1）；  
## 维度推广 ##  
接下来我们将一维的F(2,3)扩展到二维的F(2×2,3×3)，有两种扩展方式，一是通过堆叠F(2,3)来实现F(2×2,3×3)，二是通过嵌套F(2,3)来实现F(2×2,3×3)。后者计算量减小幅度更大，但前者占用内存更少。  
**通过堆叠F(2,3)来实现F(2×2,3×3)**  
[通过堆叠F(2,3)来实现F(2×2,3×3)]  
对直接卷积来说，该过程一共需要36次乘法和32次加法。

## 原理推导 ##  
https://zhuanlan.zhihu.com/p/82482351
https://www.zhihu.com/search?type=content&q=%E4%B8%AD%E5%9B%BD%E5%89%A9%E4%BD%99%E5%AE%9A%E7%90%86
https://zhuanlan.zhihu.com/p/44523169
将矩阵看作是**多项式乘法**,利用多项式的"中国剩余定理"来将卷积计算转化为求对应的**模运算逆**  
**中国剩余定理**  
**多项式的中国剩余定理/辗转相除法**  
**矩阵转化为多项式乘法**  
**Winograd步骤**  


# 三.FFT傅里叶 #
FFT只有在卷积核比较大的时候才有明显速度优势。但是CNN的卷积核一般都小于5，所以FFT用了没什么用  

傅里叶变换和逆变换的开销并不是影响最终卷积层性能的关键。因为1. 傅里叶变换是逐通道进行的，因此每个输入通道的变换实际上会被多个输出通道复用，均摊了输入的变换开销。2. 傅里叶变换是线性变换，因此每个输出通道的逆变换可以在累加每个输入通道之后做，均摊了输出的逆变换开销。3. 部署模型时，可以预先做好权值的变换。而基于傅里叶变换的卷积没有在CNN中广泛应用，有下面这些原因：1. 内存带宽需求增大：变换产生的复数、变换后变大的kernel都会增大内存带宽需求。2. 计算更复杂：如果没有专门的计算单元或者指令的话，一次复数乘法实际上需要四次实数乘加。但是，更本质的原因是，有其他更合适的方法。基于傅里叶变换的卷积本质上是一类算法的特例，这类算法被称为最优卷积算法。在一维情况下，最优卷积算法是指使用一个大小为k的卷积核对一个长度为n的序列做卷积，所需计算次数为n+k-1的所有卷积算法。以上的分析，包括用傅里叶变换来做卷积的理论支持都是数学上的分析，而实际应用中我们还必须考虑数值计算精度的影响。最优卷积算法本质上是通过精度换速度的一种方式，例如同样数值计算精度下，FFT卷积的误差通常会比直接计算更高（频域的舍入误差变换到空间域后可能会变大）。CNN模型需要的计算精度实际上很低，例如有用fp16、int8实现CNN的方法，也有用更低bit数甚至binary计算实现的方法，它们都有不错的ImageNet分类精度等。然而，这些方法通常需要硬件支持，例如fp16和int8只在一些比较新的GPU上可以用，而更低bit的方法往往需要FPGA等定制化硬件。那么在已有的通用平台上，最优卷积算法这种精度换速度的方法就非常实用。只不过，目前主要采用的是另一种最优卷积算法：Winograd卷积，它比FFT卷积更实用。Winograd变换和傅里叶变换一样，都是线性变换，但变换到实数域，因此不需要复数乘法，也就比FFT卷积更适合2×2，3×3这种小kernel卷积，缺点则是有更大的计算误差。而在专门为CNN设计的计算平台上，直接用更低精度的计算形式，使得同样的面积和功耗预算下可以堆更多的计算单元是更好的做法。所以实际上，FFT卷积没有广泛应用的原因是因为通用平台上有更合适的Winograd卷积的存在，而专用平台上直接降低运算精度是更合适的方案。不过，现在CNN里面越来越多的1×1卷积和depthwise卷积被加入，Winograd卷积的价值也越来越小了。
