https://www.bilibili.com/video/BV19J411R7t2   

## 1.量化定义 ##  
目的:优化离散变量  
(1)矩阵分解  IxJ = (IxR) * (RxJ)  主要为了内存大小占用,使用较少,计算相关问题  
(2)剪枝,将某些参数置为0,对cuda计算无加速, 结构化剪枝(例如将整个chanel去掉,可以加速,减少内存)  
(3) automl,神经网络搜索搜索出一些较小的网络  


## 2.量化挑战 ##  
正向传播:
1.精度减小,使得模型表达能力不足
2.trade-off:量化范围和精度
3.均匀量化,非均匀量化(效果好,不好实现)  
反向传播:
<img src="https://latex.codecogs.com/gif.latex?h=Q(W_{T}^{f}\times&space;x)" title="h=Q(W_{T}^{f}\times x)" />

<img src="https://latex.codecogs.com/gif.latex?y=P(h)" title="y=P(h)" />
Q为量化函数,f表示float,W为原始参数,x为输入,y为输出

对损失函数L求导:
<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{W_{f}^{T}}=\frac{\partial&space;y}{\partial&space;h}\frac{\partial&space;h}{Q(W_{f}^{T})}\frac{\partial&space;Q(W_{f}^{T})}{W_{f}^{T}}" title="\frac{\partial L}{W_{f}^{T}}=\frac{\partial y}{\partial h}\frac{\partial h}{Q(W_{f}^{T})}\frac{\partial Q(W_{f}^{T})}{W_{f}^{T}}" />

但是Q,P是离散的函数,求导值为0,所以梯度无法到达W   
方法一:强行认为Q,P的导数为1,前向使用量化函数,反向传播则视为无量化函数  
问题:straight-forward Estimator(STE)  
效果还行,网络可以训练,但是loss的走向画出可以发现波动较大,因为此方法相当于让量化后的梯度和量化前的梯度一致,是**gradient mismatch**  
另外需要注意,需要做clip,避免值太大  

量化主要处理三个部分   
1.Pre-Quant Transform,调整参数分布  
2.Projection 将之前连续的值映射到量化的维度上(关键)    
3.Post-Quant Transform 再处理/再参数化(Reparameterrization),例如乘quantization factor.  
   
   Aq = Q_1(A)
   Wq = Q_1(W)
   [Aq*r+b] * [Wq*a] //r和b通过网络训练得到,使用较小的计算量来提升网络表达能力,a是固定的,不可更新  
   = [Aq*Wq] * ar + ab[1*Wq]  //ab[1*Wq]  可以提前算出,复用  
   
   
     
量化resolution和resolution的trade-off   
参数和权重的特点,long-tail 和bell-shape的分布  

<img src="https://github.com/gxsaccount/LanguageNotes/blob/master/%E7%BB%BC%E5%90%88/%E6%B7%B1%E5%BA%A6%E9%AA%97%E9%92%B1/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/img/%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83%E7%89%B9%E6%80%A7.jpg"/>  

**精度和范围的trade-off**
<img src=https://github.com/gxsaccount/LanguageNotes/blob/master/%E7%BB%BC%E5%90%88/%E6%B7%B1%E5%BA%A6%E9%AA%97%E9%92%B1/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/img/%E7%B2%BE%E5%BA%A6%E5%92%8C%E8%8C%83%E5%9B%B4%E7%9A%84trade-off.png>
量化范围过小,中间,外围离群值( outliers)被clip掉,大值对网络影响较大,会影响结果   
量化范围过大,对 outliers优化,但是中间的值很多,量化分辨率变低,   

让lossfunc决定  
**loss-aware clipping**
<img src="https://latex.codecogs.com/gif.latex?y=PACT(x)=0.5(|x|-|x-a|&plus;a)=\left\{\begin{matrix}&space;0,x\in&space;(-\infty&space;,0))\\&space;x,x\in[0,a)]\\&space;a,x\in[a,&plus;\infty]&space;\end{matrix}\right." title="y=PACT(x)=0.5(|x|-|x-a|+a)=\left\{\begin{matrix} 0,x\in (-\infty ,0))\\ x,x\in[0,a)]\\ a,x\in[a,+\infty] \end{matrix}\right." />

 <img src="https://latex.codecogs.com/gif.latex?y_{q}=round(y\cdot&space;\frac{2^{^{k}}-1}{a})\cdot&space;\frac{a}{2^{^{k}}-1}" title="y_{q}=round(y\cdot \frac{2^{^{k}}-1}{a})\cdot \frac{a}{2^{^{k}}-1}" />
 
 <img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y_{q}}{\partial&space;a}=&space;\frac{\partial&space;y_{q}}{\partial&space;y}\frac{\partial&space;y}{\partial&space;a}=&space;\left\{\begin{matrix}&space;0,x\in(-\infty&space;,a))\\&space;1,x\in[a,&plus;\infty)&space;\end{matrix}\right." title="\frac{\partial y_{q}}{\partial a}= \frac{\partial y_{q}}{\partial y}\frac{\partial y}{\partial a}= \left\{\begin{matrix} 0,x\in(-\infty ,a))\\ 1,x\in[a,+\infty) \end{matrix}\right." />
 
 在内部(inside weights)的权重求导均为0,外部(outliers)都为1,这样只有外部的值为1时才能对梯度有影响.  
 为了获得一个合适的clipping threshold,**inside weights和outliers都应该被考虑**  
 
  <img src=https://github.com/gxsaccount/LanguageNotes/blob/master/%E7%BB%BC%E5%90%88/%E6%B7%B1%E5%BA%A6%E9%AA%97%E9%92%B1/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/img/%E8%80%83%E8%99%91%E5%86%85%E5%A4%96%E5%80%BC.jpg/>
 
 **均匀量化,非均匀量化,和改善后的非均匀量化**  
 
 <img src=https://github.com/gxsaccount/LanguageNotes/blob/master/%E7%BB%BC%E5%90%88/%E6%B7%B1%E5%BA%A6%E9%AA%97%E9%92%B1/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/img/%E6%94%B9%E5%96%84%E5%90%8E%E7%9A%84%E9%9D%9E%E5%9D%87%E5%8C%80%E9%87%8F%E5%8C%96.png/>
 
power-of-two对于硬件友好  
poewr-of-two(图b)的精度是固定的,对于0附近的像素/精度太多了,而对于大值的像素精度/太低,提出图(c),将两个power-of-two合并,将0附近的精度补偿到大值  



**第一层和最后一层一般不用量化,对于结果太敏感**  

**量化和剪枝一起做**  
Main/Subsidiy  

Bridge Accuracy Gap  
1.使用wider neural networks(简单有效),精度下降使用宽一点的网络  
2.Mixed precision weights and activation.  重要参数多bits数,不重要的bits,使用强化学习(一般硬件不同bits需要补位,性能提升不明显)  
3.Make several copies for layers ,将一层copy多分用不同的量化结果,最后在和到一块,(未能复现)  
4.集成学习,多个低bits使用集成学习方法投票,多个网络可以并行  










 KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)
校准方法:  
● 在验证集上做 FP32模型推理.  
● 对于每一层:  
○ 收集 histograms of activations.  
○ generate many quantized distributions with different saturation thresholds.  
○ 选择KL_divergence(ref_distr, quant_distr)最小的阈值.  

候选Q的分布  
1.KL_divergence(P, Q) requires that len(P) == len(Q)   
2.Candidate distribution Q is generated after merging ‘ i ’ bins from bin[0] to bin[i-1] into 128 bins  
Afterwards Q has to be ‘expanded’ again into ‘i’ bins  

Here is a simple example: reference distribution P consisting of 8 bins, we want to quantize into 2 bins:  
P = [ 1, 0, 2, 3, 5, 3, 1, 7]  
we merge into 2 bins (8 / 2 = 4 consecutive bins are merged into one bin)  
[1 + 0 + 2 + 3 , 5 + 3 + 1 + 7] = [6, 16]  
then proportionally expand back to 8 bins, we preserve empty bins from the original distribution P:  
Q = [ 6/3, 0, 6/3, 6/3, 16/4, 16/4, 16/4, 16/4] = [ 2, 0, 2, 2, 4, 4, 4, 4]  
now we should normalize both distributions, after that we can compute KL_divergence  
P /= sum(P)  
Q /= sum(Q)  
result = KL_divergence(P, Q)  
