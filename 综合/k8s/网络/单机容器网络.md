**网桥bridge：在 Linux 中，能够起到虚拟交换机作用的网络设备，它是一个工作在数据
链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端
口（Port）上。**    

Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连
接在 docker0 网桥上的容器，就可以通过它来进行通信。  

### 如何把这些容器“连接”到 docker0 网桥 ###  
需要使用**Veth Pair**的虚拟设备，
Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出
现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网
卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。  
 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。  
 
 实例：  
 1.启动了一个叫作 nginx-1 的容器：  
 
    $ docker run –d --name nginx-1 nginx  
 2.然后进入到这个容器中查看一下它的网络设备：  
 
    # 在宿主机上
    $ docker exec -it nginx-1 /bin/bash
    # 在容器里
    root@2b3c181aecf1:/# ifconfig
    eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500
         inet 172.17.0.2 netmask 255.255.0.0 broadcast 0.0.0.0
         ...

    lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536
         inet 127.0.0.1 netmask 255.0.0.0
         ...
         
    $ route
    Kernel IP routing table
    Destination Gateway Genmask Flags Metric Ref Use Iface
    default 172.17.0.1 0.0.0.0 UG 0 0 0 eth0
    172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0  
这个容器里有一张叫作 eth0 的网卡，它正是一个 Veth Pair 设备在容器里的这一
端。  
通过 route 命令查看 nginx-1 容器的路由表，我们可以看到，这个 eth0 网卡是这个容器里的
默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条
172.17.0.0 路由规则）。  

而这个 Veth Pair 设备的另一端，则在宿主机上。你可以通过查看宿主机的网络设备看到它  

      # 在宿主机上
      $ ifconfig
      ...
      docker0 Link encap:Ethernet HWaddr 02:42:d8:e4:df:c1 
       inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0
       ...
      vethb4963f3 Link encap:Ethernet HWaddr 52:81:0b:24:3d:da 
       inet6 addr: fe80::5081:bff:fe24:3dda/64 Scope:Link
       ...

      $ brctl show
      bridge name bridge id STP enabled interfaces
      docker0 8000.0242d8e4dfc1 no veth9c02e56  

通过 ifconfig 命令的输出，你可以看到，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是
一张虚拟网卡。它的名字叫作 veth9c02e56。并且，通过 brctl show 的输出，你可以看到这张
网卡被“插”在了 docker0 上。   
再在这台宿主机上启动另一个 Docker 容器，比如 nginx-2,会发现一个新的、名叫 vethb4963f3 的虚拟网卡，也被“插”在了 docker0 网桥上。  
在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址（172.17.0.3），就会发
现同一宿主机上的两个容器**默认就是相互连通的**。   

## docker网桥原理 ##     

### 图解 ###
同一宿主机不同容器通过docker0访问流程  
![9e5abe6f0edf749495bcd9bf7617003](https://user-images.githubusercontent.com/20179983/148673622-dbc6dbc7-641d-48cd-8845-9a902e86b48c.png)
在一台宿主机上，访问该宿主机上的容器的 IP 地址流程：  
![59630eb5ad4d9360d84155bd46e1769](https://user-images.githubusercontent.com/20179983/148673772-b8eb597b-181c-45de-9082-d6c885797230.png)

容器试图连接到另外一个宿主机   
![7041366bdf73ecc28dd2f3fae0ec6e9](https://user-images.githubusercontent.com/20179983/148673804-627e66f4-4114-4f1e-8b65-1e144d4c676a.png)
比如：ping 10.168.0.3，它发出的请求数
据包，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则
（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。
所以接下来，这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3
对应的宿主机上。   




###实例###

当你在 nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目
的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。可以看到，这条路由规则的网关
（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应
该经过本机的 eth0 网卡，通过二层网络直接发往目的主机。  

而要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。
所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查
找对应的 MAC 地址。  

这个 eth0 网卡，是一个 Veth Pair，它的一端在这个 nginx-1 容器的
Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在
了宿主机的 docker0 网桥上。  

一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。**从设备**会被“剥夺”调
用网络协议栈处理数据包的资格，从而“降级”成为**网桥上的一个端口**。而这个端口唯一的作
用，就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交
给对应的网桥。  

所以，在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转
发到其他被“插”在 docker0 上的虚拟网卡上。这样，同样连接在 docker0 上的 nginx-2 容
器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给
nginx-1 容器。  

有了这个目的 MAC 地址，nginx-1 容器的 eth0 网卡就可以将数据包发出去。   

而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth9c02e56
虚拟网卡上。不过，此时这个 veth9c02e56 网卡的网络协议栈的资格已经被“剥夺”，所以这
个数据包就直接流入到了 docker0 网桥里。
此时，对宿主机来说，docker0 网桥就是一个普通的网卡。  

所以，对于这个网桥上流入的数据包，就会通过宿主机的网络协议栈进行处理。   

需要注意的是，
在宿主机上，Docker 会为你设置如下所示的路由规则：   

$ route
Kernel IP routing table
Destination Gateway Genmask Flags Metric Ref Use Iface
...
172.17.0.0 * 255.255.0.0 U 0 0 0 docker0  

而这次流入的数据包的目的 IP 是 172.17.0.3，所以，当它出现宿主机之后，就会按照上述
172.17.0.0 这条路由规则，再次经过 docker0 网桥转发（FORWARD）出去。
docker0 处理转发的过程，则继续扮演二层交换机的角色。此时，docker0 网桥根据数据包的
目的 MAC 地址（也就是 nginx-2 容器的 MAC 地址），在它的 CAM 表（即交换机通过 MAC
地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：vethb4963f3，然
后把数据包发往这个端口。
而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一
个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。  

所以，nginx-2 容器看到的情况是，它自己的 eth0 网卡上出现了流入的数据包。这样，nginx-
2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。  


在实际的数据传递时，上述数据的传递过程在网络协议栈的不同层次，都有
Linux 内核 Netfilter 参与其中。所以，如果感兴趣的话，你可以通过打开 iptables 的 TRACE
功能查看到数据包的传输过程，具体方法如下所示  

    # 在宿主机上执行
    $ iptables -t raw -A OUTPUT -p icmp -j TRACE
    $ iptables -t raw -A PREROUTING -p icmp -j TRACE   

通过上述设置，你就可以在 /var/log/syslog 里看到数据包传输的日志了  
https://en.wikipedia.org/wiki/Iptables  


### 容器连不通“外网”排查 ###  

先试试 docker0 网桥能不能 ping通  
查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常  




