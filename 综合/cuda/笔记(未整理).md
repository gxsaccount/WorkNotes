## 1.并行编程的通讯模式 ##  
1.communication patterns
程序内的数据读取写入交换和内存的存储方式    
1.映射Map
    输入输出关系：一一对应，可以很方便的并行
2.聚合gather
    输入输出关系：多对一，卷积操作
3.分散scatter
    一对多，或输入少于输入，反卷积
4.模板stencil
    以固定的模式读取相邻内存数值，（如serveral-to-one）
    如读取
5.转置transpose
    一对一，类似于映射
6压缩reduce
    全连接256->全连接64->1
    all to one
7.重排
    多对多，用新的规则进行排序
## 2.GPU硬件模式 ##    
cuda程序步骤：
1.cpu分配空间给GPU（cudaMalloc）
2.CPU复制数据给GPU（cudaMemcpy）
3.CPU加载kernals给GPU做计算
4.CPU把GPU计算结果复制回来

//kernal函数

    __global__ void square(float * d_out,float* d_in){
        int idx = threadIdx.x;
        float f =d_in[dix];
        d_out[idx] = f*f;
    }
    int main(){
        const int ARRAY_SZIE = 64;
        const int ARRAY_BYTES = ARRAY_SZIE * size_of(float);

        float h_in[ARRAY_SZIE];
        for(int i =0 ; i <ARRAY_SZIE ; i++){
            h_in[i] = float(i);
        }
        float h_out[ARRAY_SZIE];

        float* d_in;
        float* d_out;

        cudaMalloc((void**) &d_in,ARRAY_BYTES);
        cudaMalloc((void**) &d_out,ARRAY_BYTES);

        cudaMemcpy(d_in,h_in,ARRAY_BYTES,cudaMemcpyHostToDevice);

        //1：创建一个线程块， ARRAY_SZIE：线程块有多少线程
        square<<<1,ARRAY_SZIE>>>(d_out,d_in);

        cudaMemcpy(h_out,d_out,ARRAY_BYTES,cudaMemcpyDeviceToHost);

        cudaFree(d_in);
        cudaFree(d_out);
    }

//编译使用nvcc，类似与gcc  

## 3.cuda编程模型 ##  
cuda特点：对线块将在何处，何时运行不保证
优点：
1.硬件真正有效运行灵活，不受硬件和逻辑的牵制
2.无需线程互相等待
3.可扩展性强
不保证：
1.对于那个快在那个SM上运行无法进行任何假设
2.无法获取快之间的明确通讯
    并行死锁
    线程退出
保证：
1.所有同一个线程块的线程必然在同一时间运行在同一个sm上
2.同一个kernal的线程块都完成之后才会运行下一个内核

内存模型：
访存速度：local memory 》 share memory 》》global memory 》 主机内训host memory
-local memory针对GPU上单线程
-share memory针对GPU上线程块
-globalmemory针对GPU的内存
-host memory主机内存
-启发：读取的数据越小，执行的线程越多，速度越快  

同步性synchronisation和屏障barrier  





GPU的硬件模式（硬件如何和软件产生联系）

1.SM（stream multiprocessor）：流处理  
有一个memory和多个simple processor组成
2.GPU每个GPU有多个流处理器，目前16个算大的，每个SM并行独立运行  
GPU将kernal的某个线程块被分配到某个流处理上，
2.kernal核：类似于c/c++的一个函数，由若干个线程块组成
3.线程块：一组线程，解决一个问题

1.cuda代码的搞笑策略
1.1 高效公式
Matk/Memory ： 数学计算量/每个线程的内存
-最大化每个线程的计算量
-最小化每个线程的内存读取速度
    -每个线程读取的数据量少
    -每个线程读取的速度快
        -local memory 》 share memory 》》 global memory（尽量放在本地内存）
        -合并 global memory（对必须放在全局内存的数据做优化）

1。2合并全局内训
            读取连续内存》读取固定规则间隔内存（如有步长）》随机读取
            g[i++]>g[i+=3]>g[random]
1.3避免线程发散
    每个线程块运行内容不一样，导致每个线程的完成时间不一样，使得线程块出现等待情况  
    如kernal函数中有**条件判断**，和每次循环次数不一样，如for j in range（i） for i in range（n） 

2.kernal加载方式
2.1查询本机参数
不要设置超过核数的线程量
2.2kernal加载的1D，2D，3D模式
2.3kernal函数关键字

3cuda中各种内存的代码使用
3.1全局内存
    
    __global__ void use_global_memory_GPU(float *array){
        array[threadIdx.x]  = 2.0f * (float) threadIdx.x;
    }
3.2共享内存

    __global__ void use_share_memory_GPU(float *array){
        array[threadIdx.x]  = 2.0f * (float) threadIdx.x;
        //需要有同步代码
        __syncthreadss();
    }
    int main(){
        //线程块，线程块的线程数，共享变量的大小
        use_share_memory_GPU<<<blocks,threads.threads*sizeof(float)>>>(d_xxx,d_in);
    }
3.3本地内存

4.cuda同步操作
4.1原子操作
加减最下异或等，不支持求余和求幂等
运行顺序不定
安排不当会使速度很慢，因为内部是串行

    __global__ void increment_atomic(int *g){
        //which thread is this?
        //threadIdx表示当前线程在block的位置，
        int i = blockIdx.x * blockDim.x + threadIdx.x;
        // each thread to increment consecutive elements , wrapping at ARRAY_SIZE
        i = i*ARRAY_SIZE;
        atomicAdd(&g[i],1);
    }
    4.2同步操作
_syncthreads():线程块内同步
_threadfence():线程调用__threadfence后,线程在_threadfence()前对global——memory或share——memory的
访问已经全部完成，执行结果对grid的所有线程可见
_threadfence_block()：线程调用__threadfence后,线程在_threadfence()前对global——memory或share——memory的
访问已经全部完成，执行结果对block中的所有线程可见
以上两个函数作用是及时通知其他线程，global——memory或share——memory的访问已经全部完成
4.3CPU/GPU同部
CPU/GPU同步：cudaStreamSynchronize()/cudaEventSynchronize()
CPU和GPU线程同同步：主机端使用cudaTreadSynchronize()
kernal启动后控制权将异步返回，利用该函数可以确定所有设备均已运行结束

5.并行化高校策略
5.1归约
5.2扫描
