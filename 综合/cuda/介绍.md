## GPU与CPU ##  
<img src=https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/gpu-devotes-more-transistors-to-data-processing.png />    
绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元.      
       
CPU(低延时):  
CPU有强大计算单元(ALU),可以在少时钟周期内完成计算.  
CPU的时钟周期频率非常高  
CPU的混存较大可以降低延时,增大命中率  
CPU的Control控制单元复杂,可以进行分支预测,数据转发(数据依赖),需要更多的对比,转发电路单元  
GPU(吞吐量):
GPU采用了数量众多的计算单元和超长的流水线.  
GPU只有非常简单的控制逻辑并省去了Cache。  
所以与CPU擅长逻辑控制和通用类型数据运算不同，GPU擅长的是大规模并发计算，这也正是密码破解等所需要的。    
所以GPU除了图像处理，也越来越多的参与到计算当中来。
## 适合的情况 ##  
　　（1）**计算密集型的程序**。所谓计算密集型(Compute-intensive)的程序，就是其大部分运行时间花在了寄存器运算上，寄存器的速度和处理器的速度相当，从寄存器读写数据几乎没有延时。可以做一下对比，读内存的延迟大概是几百个时钟周期；读硬盘的速度就不说了，即便是SSD, 也实在是太慢了。  

　　（2）**易于并行的程序**。GPU其实是一种SIMD(Single Instruction Multiple Data)架构， 他有成百上千个核，每一个核在同一时间最好能做同样的事情。  
　　满足以上两点，就可以用GPU做运算了  
## 三个关键抽象 ##  
**线程组**，**共享内存**和**屏障同步**的层次结构.  
这些抽象提供细粒度数据并行和线程并行，嵌套在粗粒度数据并行和任务并行中。  

## GPU的自动伸缩性 ##  
<img src= https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png/>  
GPU是围绕一组流式多处理器(Streaming Multiprocessors SMs)构建的(有关更多细节，请参见<a text=硬件实现 src= https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation/>)。一个多线程程序被分割成独立执行的线程块，因此一个多处理器的GPU比一个少处理器的GPU在更短的时间内自动执行程序。
v_out


#CUDA中grid、block、thread、warp与SM、SP的关系#

首先概括一下这几个概念。其中SM（Streaming Multiprocessor）和SP（streaming Processor）是硬件层次的，其中一个SM可以包含多个SP。thread是一个线程，多个thread组成一个线程块block，多个block又组成一个线程网格grid。
对于 Nvidia 的 GPU 的编程语言 CUDA 来说，它的语言设计逻辑上和硬件层次是对应的。CUDA 有三层逻辑层次，分别为 grid，block，和 thread。Grid 可以认为是对整个显卡的逻辑抽象，block 可以认为是对 SM 单元的逻辑抽象，而 thread 是 SP 的逻辑抽象。为了达到最高的并发程度，SM 之间可以认为是没有交互的，当然这也不是绝对的，有一些程序为了自己的特殊逻辑，也可以设计出 SM 之间依赖的程序，但这个代价是极大的性能浪费。
现在就说一下一个kenerl函数是怎么执行的。一个kernel程式会有一个grid，grid底下又有数个block，每个block是一个thread群组。在同一个block中thread可以通过共享内存（shared memory）来通信，同步。不同的block 之间的数据通信要通过 global memory
CUDA的设备在实际执行过程中，会以block为单位。把一个个block分配给SM进行运算；而block中的thread又会以warp（线程束）为单位，对thread进行分组计算。目前CUDA的warp大小都是32，也就是说32个thread会被组成一个warp来一起执行。同一个warp中的thread执行的指令是相同的，只是处理的数据不同。

基本上warp 分组的动作是由SM 自动进行的，会以连续的方式来做分组。比如说如果有一个block 里有128 个thread 的话，就会被分成四组warp，第0-31 个thread 会是warp 1、32-63 是warp 2、64-95是warp 3、96-127 是warp 4。而如果block 里面的thread 数量不是32 的倍数，那他会把剩下的thread独立成一个warp；比如说thread 数目是66 的话，就会有三个warp：0-31、32-63、64-65 。由于最后一个warp 里只剩下两个thread，所以其实在计算时，就相当于浪费了30 个thread 的计算能力；这点是在设定block 中thread 数量一定要注意的事！

一个SM 一次只会执行一个block 里的一个warp，但是SM 不见得会一次就把这个warp 的所有指令都执行完；当遇到正在执行的warp 需要等待的时候（例如存取global memory 就会要等好一段时间），就切换到别的warp来继续做运算，借此避免为了等待而浪费时间。所以理论上效率最好的状况，就是在SM 中有够多的warp 可以切换，让在执行的时候，不会有「所有warp 都要等待」的情形发生；因为当所有的warp 都要等待时，就会变成SM 无事可做的状况了。

实际上，warp 也是CUDA 中，每一个SM 执行的最小单位；如果GPU 有16 组SM 的话，也就代表他真正在执行的thread 数目会是32*16 个。 不过由于CUDA 是要透过warp 的切换来隐藏thread 的延迟、等待 ，来达到大量平行化的目的 ，所以会用所谓的 active thread 这个名词来代表一个SM 里同时可以处理的thread 数目。而在block 的方面， 一个SM 可以同时处理多个thread block，当其中有block 的所有thread 都处理完后，他就会再去找其他还没处理的block 来处理 。假设有16 个SM、64 个block、每个SM 可以同时处理三个block 的话，那一开始执行时，device 就会同时处理48 个block； 而剩下的16 个block 则会等SM 有处理完block 后，再进到SM 中处理，直到所有block 都处理结束
